# ==============================================================
# 7120CEM Coursework 1: Offensive Language Detection in Tweets
# Author: Devika [Your Surname]
# Date: November 2025
# ==============================================================
# This notebook implements Logistic Regression, Linear SVM,
# and Tuned RBF SVM models for detecting offensive language
# using the OffensEval 2019 English dataset.
# ==============================================================


# ==============================================================
# 1. Import Required Libraries
# ==============================================================
import pandas as pd
import numpy as np
import re
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.svm import LinearSVC, SVC
from sklearn.metrics import classification_report, confusion_matrix


# ==============================================================
# 2. Load and Inspect Dataset
# ==============================================================
# Load the OffensEval dataset (TSV format)
df = pd.read_csv('offenseval.tsv', sep='\t')

# Preview structure
print(df.head())
print("\nColumns:", df.columns)

# Check label distribution
label_counts = df['subtask_a'].value_counts()
print("\nLabel counts:\n", label_counts)
print("\nLabel percentages:\n", (label_counts / len(df) * 100))

# Rename for simplicity
df = df.rename(columns={'tweet': 'text', 'subtask_a': 'label'})


# ==============================================================
# 3. Data Preprocessing
# ==============================================================
def clean_text(text):
    """Clean tweet text by removing noise and normalising."""
    text = text.lower()
    text = re.sub(r'https?://\S+', '', text)      # Remove URLs
    text = re.sub(r'@\w+', '', text)              # Remove mentions
    text = re.sub(r'[^a-z\s]', ' ', text)         # Remove special characters
    text = re.sub(r'\s+', ' ', text).strip()      # Remove extra spaces
    return text

# Apply cleaning
df['clean_text'] = df['text'].apply(clean_text)

# Split dataset (80% train / 20% test, stratified)
X_train, X_test, y_train, y_test = train_test_split(
    df['clean_text'],
    df['label'],
    test_size=0.2,
    random_state=42,
    stratify=df['label']
)


# ==============================================================
# 4. Feature Extraction (TF-IDF)
# ==============================================================
vectorizer = TfidfVectorizer(
    ngram_range=(1, 2),        # unigrams + bigrams
    min_df=5,
    max_df=0.95,
    sublinear_tf=True
)

X_train_vec = vectorizer.fit_transform(X_train)
X_test_vec = vectorizer.transform(X_test)

print("TF-IDF feature space:", X_train_vec.shape)


# ==============================================================
# 5. Model 1 — Logistic Regression
# ==============================================================
lr = LogisticRegression(max_iter=300, solver='liblinear')
lr.fit(X_train_vec, y_train)
lr_pred = lr.predict(X_test_vec)

print("\nLogistic Regression Results:\n")
print(classification_report(y_test, lr_pred))


# ==============================================================
# 6. Model 2 — Linear SVM
# ==============================================================
linear_svm = LinearSVC(C=1.0)
linear_svm.fit(X_train_vec, y_train)
linear_pred = linear_svm.predict(X_test_vec)

print("\nLinear SVM Results:\n")
print(classification_report(y_test, linear_pred))


# ==============================================================
# 7. Model 3 — RBF SVM (with Grid Search Tuning)
# ==============================================================
param_grid = {
    'C': [0.1, 1, 10],
    'gamma': [1, 0.1, 0.01, 0.001]
}

rbf_svm = SVC(kernel='rbf')

grid = GridSearchCV(
    rbf_svm,
    param_grid,
    cv=3,
    scoring='f1_macro',
    verbose=2
)

grid.fit(X_train_vec, y_train)

print("\nBest Parameters:", grid.best_params_)
print("Best Cross-Validation F1:", grid.best_score_)

# Predict with the best model
best_svm = grid.best_estimator_
best_pred = best_svm.predict(X_test_vec)

print("\nTuned RBF SVM Results:\n")
print(classification_report(y_test, best_pred))


# ==============================================================
# 8. Confusion Matrices for All Models
# ==============================================================
models = {
    "Logistic Regression": lr_pred,
    "Linear SVM": linear_pred,
    "Tuned RBF SVM (C=10, γ=0.1)": best_pred
}

for name, preds in models.items():
    cm = confusion_matrix(y_test, preds)
    plt.figure(figsize=(5,4))
    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues',
                xticklabels=['NOT', 'OFF'], yticklabels=['NOT', 'OFF'])
    plt.xlabel('Predicted')
    plt.ylabel('Actual')
    plt.title(f'Confusion Matrix — {name}')
    plt.tight_layout()
    plt.show()


# ==============================================================
# 9. Class Distribution Chart
# ==============================================================
sns.countplot(x='label', data=df, palette='coolwarm')
plt.title('Class Distribution: NOT vs OFF')
plt.xlabel('Label')
plt.ylabel('Count')
plt.tight_layout()
plt.show()


# ==============================================================
# 10. Summary
# ==============================================================
print("✔ Logistic Regression, Linear SVM, and Tuned RBF SVM trained successfully.")
print("✔ Best model: RBF SVM (C=10, γ=0.1) — Accuracy ≈ 0.75, Macro F1 ≈ 0.69.")
print("✔ Outputs visualised via confusion matrices and label distribution chart.")
